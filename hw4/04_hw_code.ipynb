{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 22:13:24.544357: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-21 22:13:24.567977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-21 22:13:24.944150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense,Embedding,Input,Dropout,Conv1D\n",
    "from tensorflow.keras.layers import SpatialDropout1D, Flatten,LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Numpy arrays\n",
    "FS_labels = np.load('./data/FS_labels.npy', allow_pickle=True)\n",
    "FS_test = np.load('./data/FS_test.npy', allow_pickle=True)\n",
    "FS_train = np.load('./data/FS_train.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training DataFrame\n",
    "train_df = pd.DataFrame(FS_train, columns=[\"text\"])\n",
    "train_df['label'] = FS_labels\n",
    "\n",
    "# Create the test DataFrame\n",
    "test_df = pd.DataFrame(FS_test, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding index from file in .txt format. First line contains \n",
    "# dictionary size and embedding dim. Fields are space separated\n",
    "def get_embeddings(file_name):\n",
    "    embeddings_index = {}\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) > 2:\n",
    "                embeddings_index[values[0]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "    return embeddings_index\n",
    "embeddings_path = \"./crawl-300d-2M.vec\"\n",
    "embeddings_index = get_embeddings(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "trans_table = str.maketrans({key: ' ' for key in string.digits + '\\r\\n' +\n",
    "                             string.punctuation.replace(\"\\'\",'')})\n",
    "def preprocess(text):\n",
    "    return ' '.join(text.lower().translate(trans_table).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 5\n",
      "27005 top words\n"
     ]
    }
   ],
   "source": [
    "### PROCESS TRAINING DATA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "UNKNOWN_PROXY = 'unknown'\n",
    "MIN_WORD_OCCURRENCE = 5\n",
    "\n",
    "train_df['description'] = train_df.text.apply(preprocess)\n",
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", \n",
    "                             min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(train_df.description)\n",
    "\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(UNKNOWN_PROXY)\n",
    "print(len(top_words),'top words')\n",
    "\n",
    "\n",
    "### APPLY TO TEST DATA\n",
    "test_df['description'] = test_df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 out of \"top_words\": \n",
      " ['streamlined', 'misleading', 'outposts', 'termination', 'steamed', 'inscrutable', 'acknowledgement', 'elders', 'labyrinths', 'beheld']\n",
      "\n",
      "Is \"unknown\" in top_words? \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "print('First 10 out of \"top_words\": \\n',list(top_words)[:10])\n",
    "print('\\nIs \"unknown\" in top_words? \\n','unknown' in top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknown(text, vocabulary, proxy):\n",
    "    return ' '. \\\n",
    "join([w if w in vocabulary else proxy for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['description'] = train_df.description.apply(filter_unknown,\n",
    "                args=(set(embeddings_index.keys() & top_words), \\\n",
    "                      UNKNOWN_PROXY))\n",
    "\n",
    "test_df['description'] = test_df.description.apply(filter_unknown,\n",
    "                args=(set(embeddings_index.keys() & top_words), \\\n",
    "                      UNKNOWN_PROXY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x7f8fc8a83b50>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(train_df.description)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('unknown', 1),\n",
       " ('the', 2),\n",
       " ('and', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('a', 6),\n",
       " ('in', 7),\n",
       " ('that', 8),\n",
       " ('he', 9),\n",
       " ('i', 10),\n",
       " ('it', 11),\n",
       " ('for', 12),\n",
       " ('his', 13),\n",
       " ('was', 14),\n",
       " ('is', 15),\n",
       " ('with', 16)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "list(word_index.items())[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 sequences in `seq`:  [[1, 1, 1, 1, 1, 1, 1, 57, 50, 17, 10984, 7, 2, 161, 1, 57, 50, 17, 4011, 1, 57, 50, 17, 11749, 1, 41, 16393, 1, 8912, 1, 41, 19723, 1, 1, 57, 42, 747, 11750, 35, 7, 2, 456, 4, 42, 161, 1, 1, 1807, 1886, 41, 1807, 5296, 50, 17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 59, 1501, 41, 59, 5060, 9595, 36, 17, 98, 4, 79, 60, 1, 41, 4, 23, 71, 105, 142, 1, 51, 112, 47, 36, 103, 13, 128, 1886, 3, 13, 128, 3242, 1, 3, 36, 821, 246, 13, 248, 7, 32, 108, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63, 236, 62, 33, 257, 71, 2942, 169, 1, 3, 2038, 1, 3, 3687, 49, 112, 60, 305, 49, 23, 13, 620, 1, 291, 203, 62, 2326, 1, 1, 1, 1, 12, 62, 86, 2326, 2, 1263, 4, 2, 140, 4, 109, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 24, 94, 377, 75, 1, 5, 489, 7, 71, 620, 1, 46, 143, 18, 24, 405, 7, 2, 161, 34, 62, 6079, 49, 90, 484, 1], [1, 1, 1, 1, 1, 1, 1, 3, 10, 48, 175, 6, 2327, 217, 37, 5, 319, 1, 1, 3, 38, 961, 25, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1]]\n",
      "\n",
      "Shape of `data`:  (180000, 50)\n",
      "\n",
      "First prepared text in `data`: [    1     1     1     1     1     1     1    57    50    17 10984     7\n",
      "     2   161     1    57    50    17  4011     1    57    50    17 11749\n",
      "     1    41 16393     1  8912     1    41 19723     1     1    57    42\n",
      "   747 11750    35     7     2   456     4    42   161     1     1  1807\n",
      "  1886    41]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "seq = tokenizer.texts_to_sequences(train_df.description)\n",
    "data = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                     truncating='post')\n",
    "with open('./text_data.pkl','wb') as f: pickle.dump(data, f, -1)\n",
    "\n",
    "seq_test = tokenizer.texts_to_sequences(test_df.description)\n",
    "data_test = pad_sequences(seq_test,maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "with open('./text_test_data.pkl','wb') as f: pickle.dump(data, f, -1)\n",
    "\n",
    "print('\\nFirst 2 sequences in `seq`: ',seq[:2])\n",
    "print('\\nShape of `data`: ',data.shape)\n",
    "print('\\nFirst prepared text in `data`:',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_1\n",
      "word_2\n",
      "word_3\n",
      "enough\n",
      "enough\n"
     ]
    }
   ],
   "source": [
    "mlist=['word_1','word_2','word_3']\n",
    "moveIter=iter(mlist)\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))\n",
    "print(next(moveIter,'enough'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dim = len(next(iter(embeddings_index.values())))\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "def get_embedding_matrix(word_index,embeddings_index):\n",
    "    nb_words = len(word_index) + 1 # +1 since min(word_index.values())=1\n",
    "    embedding_matrix = np.zeros((nb_words,embeddings_dim))\n",
    "    unknown = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None: unknown += 1\n",
    "        else: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 unknown words\n"
     ]
    }
   ],
   "source": [
    "# Create embedding_layer and save it.\n",
    "def make_save_emb_layer(word_index,embeddings_index,layer_file_name):\n",
    "    embedding_matrix,unknown = get_embedding_matrix(word_index,embeddings_index)\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],trainable=False)\n",
    "    with open(layer_file_name,'wb') as f: \n",
    "        pickle.dump(embedding_layer, f, -1)\n",
    "    return unknown\n",
    "\n",
    "EMBEDDING_LAYER_FILE = './text_embed_layer.pkl'\n",
    "print(make_save_emb_layer(word_index,embeddings_index,EMBEDDING_LAYER_FILE),\n",
    "      'unknown words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "def get_more_complex_model():\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    \n",
    "    # Stack multiple LSTM layers\n",
    "    x = LSTM(50, return_sequences=True)(x)\n",
    "    x = LSTM(50, return_sequences=True)(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    x = BatchNormalization()(conc)\n",
    "    x = Dropout(0.5)(x)  # Increased dropout rate\n",
    "    \n",
    "    # Multiple dense layers\n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='RMSprop')  # Changed optimizer\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "#     input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#     x = embedding_layer(input_layer)\n",
    "#     x = SpatialDropout1D(0.5)(x)\n",
    "#     x = LSTM(10, return_sequences=True)(x)\n",
    "#     x = Conv1D(5, kernel_size=2, padding=\"valid\")(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(.2)(x)\n",
    "#     output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=Adam())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.array(train_df['label'])\n",
    "\n",
    "with open(EMBEDDING_LAYER_FILE, 'rb') as f: \n",
    "    embedding_layer = pickle.load(f)\n",
    "\n",
    "with open('./text_data.pkl', 'rb') as f: \n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Removed stratify parameter for a random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_array, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 22:20:42.333074: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 28.83MiB (rounded to 30235392)requested by op AddV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-10-21 22:20:42.333089: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-10-21 22:20:42.333093: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 5, Chunks in use: 4. 1.2KiB allocated for chunks. 1.0KiB in use in bin. 32B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333096: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333097: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333099: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.2KiB allocated for chunks. 2.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333101: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333103: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333104: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333106: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333107: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333109: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333110: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333112: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333114: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333115: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333117: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333118: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333121: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 58.87MiB allocated for chunks. 58.87MiB in use in bin. 57.67MiB client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333122: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333126: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333127: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333129: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-10-21 22:20:42.333132: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 28.83MiB was 16.00MiB, Chunk State: \n",
      "2023-10-21 22:20:42.333135: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 61732608\n",
      "2023-10-21 22:20:42.333139: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8e2c000000 of size 256 next 2\n",
      "2023-10-21 22:20:42.333141: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8e2c000100 of size 256 next 3\n",
      "2023-10-21 22:20:42.333142: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8e2c000200 of size 256 next 4\n",
      "2023-10-21 22:20:42.333143: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8e2c000300 of size 256 next 5\n",
      "2023-10-21 22:20:42.333145: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8e2c000400 of size 30235392 next 6\n",
      "2023-10-21 22:20:42.333146: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f8e2dcd5f00 of size 256 next 7\n",
      "2023-10-21 22:20:42.333148: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8e2dcd6000 of size 31495936 next 18446744073709551615\n",
      "2023-10-21 22:20:42.333149: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 2304\n",
      "2023-10-21 22:20:42.333150: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f8eba800000 of size 2304 next 18446744073709551615\n",
      "2023-10-21 22:20:42.333152: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-10-21 22:20:42.333156: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 256 totalling 1.0KiB\n",
      "2023-10-21 22:20:42.333158: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2304 totalling 2.2KiB\n",
      "2023-10-21 22:20:42.333159: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 30235392 totalling 28.83MiB\n",
      "2023-10-21 22:20:42.333161: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 31495936 totalling 30.04MiB\n",
      "2023-10-21 22:20:42.333163: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 58.87MiB\n",
      "2023-10-21 22:20:42.333165: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 61734912 memory_limit_: 61734912 available bytes: 0 curr_region_allocation_bytes_: 246939648\n",
      "2023-10-21 22:20:42.333168: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                        61734912\n",
      "InUse:                        61734656\n",
      "MaxInUse:                     61734912\n",
      "NumAllocs:                           8\n",
      "MaxAllocSize:                 31495936\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-10-21 22:20:42.333171: W tensorflow/tsl/framework/bfc_allocator.cc:497] **************************************************************************************************x*\n",
      "2023-10-21 22:20:42.340282: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL _NotOkStatusException REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/eager/core.py(36): __init__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py(461): add_v2\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py(4022): add\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py(1176): op_dispatch_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/stateless_random_ops.py(523): stateless_random_uniform\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py(1176): op_dispatch_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/backend.py(2101): random_uniform\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/initializers/initializers.py(346): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py(1952): _init_from_args\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py(1768): __init__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(289): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variable_scope.py(2707): default_variable_creator\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(219): <lambda>\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(226): _variable_v1_call\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(285): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer_utils.py(134): make_variable\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/trackable/base.py(489): _add_variable_with_custom_getter\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(712): add_weight\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/layers/core/embedding.py(179): build\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/utils/tf_utils.py(369): wrapper\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(3023): _maybe_build\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(2476): _infer_output_signature\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(2419): _keras_tensor_symbolic_call\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(2572): _functional_construction_call\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(1058): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py(65): error_handler\n  /tmp/ipykernel_78501/1197382534.py(5): get_more_complex_model\n  /tmp/ipykernel_78501/632413256.py(7): <module>\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3508): run_code\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3448): run_ast_nodes\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3269): run_cell_async\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3064): _run_cell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3009): run_cell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/zmqshell.py(540): run_cell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(422): do_execute\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(729): execute_request\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(409): dispatch_shell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(502): process_one\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(513): dispatch_queue\n  /usr/lib/python3.10/asyncio/events.py(80): _run\n  /usr/lib/python3.10/asyncio/base_events.py(1909): _run_once\n  /usr/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tornado/platform/asyncio.py(195): start\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelapp.py(725): start\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/traitlets/config/application.py(1043): launch_instance\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel_launcher.py(17): <module>\n  /usr/lib/python3.10/runpy.py(86): _run_code\n  /usr/lib/python3.10/runpy.py(196): _run_module_as_main\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m model_checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(best_model_path,\n\u001b[1;32m      6\u001b[0m                                    save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m get_more_complex_model()\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[1;32m      9\u001b[0m plot_model(model, to_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwine_reviews.png\u001b[39m\u001b[39m'\u001b[39m,show_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,show_layer_names\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m, in \u001b[0;36mget_more_complex_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_more_complex_model\u001b[39m():\n\u001b[1;32m      4\u001b[0m     input_layer \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(MAX_SEQUENCE_LENGTH,))\n\u001b[0;32m----> 5\u001b[0m     x \u001b[39m=\u001b[39m embedding_layer(input_layer)\n\u001b[1;32m      6\u001b[0m     x \u001b[39m=\u001b[39m SpatialDropout1D(\u001b[39m0.5\u001b[39m)(x)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Stack multiple LSTM layers\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/backend.py:2101\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2099\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[1;32m   2100\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2101\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[1;32m   2102\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   2103\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[1;32m   2104\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[1;32m   2105\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2106\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   2107\u001b[0m     )\n\u001b[1;32m   2108\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[1;32m   2109\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[1;32m   2110\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2114\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL _NotOkStatusException REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/eager/core.py(36): __init__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py(461): add_v2\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py(4022): add\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py(1176): op_dispatch_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/stateless_random_ops.py(523): stateless_random_uniform\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py(1176): op_dispatch_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/backend.py(2101): random_uniform\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/initializers/initializers.py(346): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py(1952): _init_from_args\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py(1768): __init__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(289): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variable_scope.py(2707): default_variable_creator\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(219): <lambda>\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(226): _variable_v1_call\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py(285): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer_utils.py(134): make_variable\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tensorflow/python/trackable/base.py(489): _add_variable_with_custom_getter\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(712): add_weight\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/layers/core/embedding.py(179): build\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/utils/tf_utils.py(369): wrapper\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(3023): _maybe_build\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(2476): _infer_output_signature\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(2419): _keras_tensor_symbolic_call\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(2572): _functional_construction_call\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/engine/base_layer.py(1058): __call__\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py(65): error_handler\n  /tmp/ipykernel_78501/1197382534.py(5): get_more_complex_model\n  /tmp/ipykernel_78501/632413256.py(7): <module>\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3508): run_code\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3448): run_ast_nodes\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3269): run_cell_async\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3064): _run_cell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3009): run_cell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/zmqshell.py(540): run_cell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/ipkernel.py(422): do_execute\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(729): execute_request\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(409): dispatch_shell\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(502): process_one\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelbase.py(513): dispatch_queue\n  /usr/lib/python3.10/asyncio/events.py(80): _run\n  /usr/lib/python3.10/asyncio/base_events.py(1909): _run_once\n  /usr/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/tornado/platform/asyncio.py(195): start\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel/kernelapp.py(725): start\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/traitlets/config/application.py(1043): launch_instance\n  /home/henry/Desktop/MSiA/msia_env/lib/python3.10/site-packages/ipykernel_launcher.py(17): <module>\n  /usr/lib/python3.10/runpy.py(86): _run_code\n  /usr/lib/python3.10/runpy.py(196): _run_module_as_main\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model_WOW.h5'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "model_checkpoint = ModelCheckpoint(best_model_path,\n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "model = get_more_complex_model()\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='wine_reviews.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train,validation_data\u001b[39m=\u001b[39m(X_test, y_test),\n\u001b[1;32m      2\u001b[0m                  epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m      3\u001b[0m                  callbacks\u001b[39m=\u001b[39m[model_checkpoint])\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mload_weights(best_model_path)\n\u001b[1;32m      5\u001b[0m test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train,validation_data=(X_test, y_test),\n",
    "                 epochs=50, batch_size=BATCH_SIZE, shuffle=True, verbose=2,\n",
    "                 callbacks=[model_checkpoint])\n",
    "model.load_weights(best_model_path)\n",
    "test_pred = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "print('validation AUC',roc_auc_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mplot(hist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(hist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mmodel loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get submission predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(data_test, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round predictions to the nearest integer (0 or 1)\n",
    "rounded_test_pred = np.round(test_pred).astype(int)\n",
    "\n",
    "# Save the rounded predictions to a CSV file\n",
    "np.savetxt('submission.csv', rounded_test_pred, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
